package MaastCTS2.controller;

import java.util.ArrayList;
import java.util.HashMap;

import MaastCTS2.Agent;
import MaastCTS2.Globals;
import MaastCTS2.KnowledgeBase;
import MaastCTS2.gnu.trove.list.array.TIntArrayList;
import MaastCTS2.heuristics.states.IPlayoutEvaluation;
import MaastCTS2.model.ActionLocation;
import MaastCTS2.model.ActionNGram;
import MaastCTS2.model.MctNode;
import MaastCTS2.model.Score;
import MaastCTS2.move_selection.IMoveSelectionStrategy;
import MaastCTS2.playout.IPlayoutStrategy;
import MaastCTS2.selection.ISelectionStrategy;
//import MaastCTS2.utils.MctsVisualizer;
import core.competition.CompetitionParameters;
import core.game.StateObservationMulti;
import ontology.Types.ACTIONS;
import ontology.Types.WINNER;
import tools.ElapsedCpuTimer;

public class MctsController implements IController {
	
	/** Buffer (in milliseconds) of the thinking time per cycle that will not be used (to make sure that an action is returned in time) */
	public static int TIME_BUFFER_MILLISEC = 10;
	
	/** 
	 * The maximum number of states we'll generate per action in the level below root for safety prepruning 
	 */
	public static int MAX_NUM_SAFETY_CHECKS = 3;

	/** Total number of iterations of the main loop of MCTS in an entire match */
	public static int TOTAL_ITERATIONS;
	/** The minimum number of iterations of the main loop of MCTS in a single cycle of the match */
	public static int MIN_ITERATIONS_PER_GAME;
	/** The maximum number of iterations of the main loop of MCTS in a single cycle of the match */
	public static int MAX_ITERATIONS_PER_GAME;
	/** Total number of iterations of the main loop of MCTS that ended in a loss in an entire match */
	//public static int TOTAL_LOSS_ITERATIONS;
	
	/** The minimum score found so far in the entire game */
	public double MIN_SCORE;
	/** The maximum score found so far in the entire game */
	public double MAX_SCORE;
	
	/** The minimum action score found so far in the entire game */
	public double MIN_ACTION_SCORE;
	/** The maximum action score found so far in the entire game */
	public double MAX_ACTION_SCORE;
	
	private double avgSimulationTime;
	
	/** The root node of our MCTS tree */
	private MctNode root;
	
	/** The current game tick of the root node */
	public int rootTick;
	
	/** The score in the root game state */
	private double rootScore;
	
	/** Strategy for selecting a node in the selection step of MCTS */
	private ISelectionStrategy selectionStrategy;
	/** Strategy for selecting an action in the playout step of MCTS */
	private IPlayoutStrategy playoutStrategy;
	
	/** The strategy for selecting the move to play after the search process */
	private IMoveSelectionStrategy moveSelectionStrategy;
	
	/** The function to use for evaluating playouts */
	private IPlayoutEvaluation playoutEval;
	
	/** The last action that we have played in real gameplay (not in Monte-Carlo simulations) */
	private ACTIONS lastAction = ACTIONS.ACTION_NIL;
	private double treeDecayFactor;
	
	/** 
	 * If true, initializes a shallow tree using breadth-first search at the start of the game, 
	 * instead of starting MCTS with only a root node.
	 */
	private boolean initBreadthFirst;
	
	/** If true, we'll collect Action statistics (for example for use in Progressive History / MAST) */
	private boolean collectActionStatistics;
	
	/** 
	 * We'll collect statistics for n-grams of actions of all sizes n where 2 <= n <= maxActionNGramSize 
	 * <br> (not using n-grams with n = 1 because we'll use the less computationally expensive actionStatistics EnumMap for that )
	 */
	private int maxActionNGramSize;
	
	/** 
	 * Table of action statistics for use in Progressive History / MAST
	 */
	private HashMap<ActionLocation, Score> actionStatistics;
	
	/**
	 * Table of action n-gram statistics for use in NST
	 */
	private HashMap<ActionNGram, Score> actionNGramStatistics;
	
	/** Factor with which to decay action statistics */
	private double actionDecayFactor = 0.8;
	
	private boolean knowledgeBasedEval;
	private boolean alwaysKB;
	
	public static int NUM_ADVANCE_OPS;
	
	/** 
	 * The very first state generated by the selection step is evaluated, and the corresponding evaluation is stored here 
	 * this is done to give a small punishment to very short-term decisions (such as staying in the same place for too long)
	 */
	public static double ONE_STEP_EVAL = 0.0;
	
	private boolean treeReuse;	
	private boolean noTreeReuseBFTI;

	public MctsController(ISelectionStrategy selectionStrategy, 
							IPlayoutStrategy playoutStrategy, IMoveSelectionStrategy moveSelectionStrategy, 
							IPlayoutEvaluation playoutEval, boolean initBreadthFirst, boolean noveltyBasedPruning,
							boolean exploreLosses, boolean knowledgeBasedEval, boolean treeReuse, double treeReuseGamma,
							int maxNumSafetyChecks, boolean alwaysKB, boolean noTreeReuseBFTI) {
		this.selectionStrategy = selectionStrategy;
		this.playoutStrategy = playoutStrategy;
		this.moveSelectionStrategy = moveSelectionStrategy;
		this.playoutEval = playoutEval;
		this.initBreadthFirst = initBreadthFirst;
		this.knowledgeBasedEval = knowledgeBasedEval;
		this.treeReuse = treeReuse;
		treeDecayFactor = treeReuseGamma;
		MAX_NUM_SAFETY_CHECKS = maxNumSafetyChecks;
		this.alwaysKB = alwaysKB;
		this.noTreeReuseBFTI = noTreeReuseBFTI;
	}

	@Override
	public void init(StateObservationMulti so, ElapsedCpuTimer elapsedTimer) {
		Globals.knowledgeBase = new KnowledgeBase();
		Globals.knowledgeBase.init(so);
		
		root = null;
		
		selectionStrategy.init(so, elapsedTimer);
		playoutStrategy.init(so, elapsedTimer, this);
		
		TOTAL_ITERATIONS = 0;
		MIN_ITERATIONS_PER_GAME = Integer.MAX_VALUE;
		MAX_ITERATIONS_PER_GAME = Integer.MIN_VALUE;
		//TOTAL_LOSS_ITERATIONS = 0;
		
		// these initial values look weird but they're not a mistake
		MIN_SCORE = Globals.HUGE_ENDGAME_SCORE;
		MAX_SCORE = -Globals.HUGE_ENDGAME_SCORE;
		
		MIN_ACTION_SCORE = Globals.HUGE_ENDGAME_SCORE;
		MAX_ACTION_SCORE = -Globals.HUGE_ENDGAME_SCORE;
		
		maxActionNGramSize = Math.max
				(selectionStrategy.getDesiredActionNGramSize(), playoutStrategy.getDesiredActionNGramSize());
		
		collectActionStatistics = (
				selectionStrategy.wantsActionStatistics() 	||
				playoutStrategy.wantsActionStatistics()		||
				maxActionNGramSize > 0);
		
		if(collectActionStatistics){
			actionStatistics = new HashMap<ActionLocation, Score>();
			
			if(maxActionNGramSize > 1){
				actionNGramStatistics = new HashMap<ActionNGram, Score>();
			}
		}
		
		// done with initializing. We'll use the remaining time to start a nice, long MCTS
		
		// we'll do this with a longer than normal time buffer just to be safe, cache the normal buffer here so we can re-set it at the end again
		int normalTimeBuffer = MctsController.TIME_BUFFER_MILLISEC;
		MctsController.TIME_BUFFER_MILLISEC = Math.max(50, normalTimeBuffer);
		
		// run MCTS
		runMcts(so, elapsedTimer);
		
		//MctsVisualizer.generateGraphFile(root);
		
		// set normal time buffer again
		MctsController.TIME_BUFFER_MILLISEC = normalTimeBuffer;
		
		// resetting these again here to avoid having the MCTS during init messing with these numbers
		TOTAL_ITERATIONS = 0;
		MIN_ITERATIONS_PER_GAME = Integer.MAX_VALUE;
		MAX_ITERATIONS_PER_GAME = Integer.MIN_VALUE;
		//TOTAL_LOSS_ITERATIONS = 0;
	}
	
	@Override
	public ACTIONS chooseAction(StateObservationMulti currentStateObs, ElapsedCpuTimer elapsedTimer){
		lastAction = runMcts(currentStateObs, elapsedTimer);
		//System.out.println("playing " + lastAction);
		//System.out.println("Avg. score of root = " + Globals.normalise(root.getTotalScore() / root.getNumVisits(), MIN_SCORE, MAX_SCORE));
		return lastAction;
	}

	public ACTIONS runMcts(StateObservationMulti rootStateObs, ElapsedCpuTimer elapsedForSimulationTimer) {
		double oldRootScore = rootScore;
		rootScore = rootStateObs.getGameScore(Agent.myID);
		rootTick = rootStateObs.getGameTick();
		
		if(root == null){
			root = new MctNode();
		}
		else{		// we can reuse a part of the tree from the previous search
			if(rootStateObs.getGameTick() != 0){
				// one important exception: if our current game tick is 0, it means that we want to re-use the tree
				// we've been generating during init. This means we do not need to find a matching child or decay
				
				boolean foundChildToReuse = false;
				
				if(treeReuse){
					for(MctNode child : root.getChildren()){
						if(child.getActionFromParent() == rootStateObs.getAvatarLastAction(Agent.myID)){
							root = child;
							root.resetParent();
							
							// some previously cached states in this node may be invalid
							// this should only be necessary in stochastic games. However, the
							// Digdug game appears to be classified as deterministic games and
							// can cause crashes if this is not done here, so that game might in
							// fact be stochastic
							root.removeCachedStates();	
							
							// decay tree
							decayOldTree(root);
							
							foundChildToReuse = true;
							break;
						}
					}
				}
				
				decayActionStatistics(rootScore > oldRootScore ? 0.0 : actionDecayFactor);
				
				if(!foundChildToReuse){
					// we didn't find a child node matching the played action, so should just start a new tree
					root = new MctNode();
				}
			}
		}
		
		Globals.knowledgeBase.updateRoot(rootStateObs);
		
		if(initBreadthFirst){
			if(!root.isFullyExpanded()){
				// perform a shallow breadth-first search to build up an initial MCTS tree with only safe actions at the root
				generateBreadthFirstTree(rootStateObs, elapsedForSimulationTimer);
			}
			else if(!noTreeReuseBFTI && rootStateObs.getGameTick() != 0){
				// root is already fully expanded, but should still do safety prepruning
				safetyPreprune(root, rootStateObs, elapsedForSimulationTimer);
			}
		}
		
		/*String actionsString = "[";
		for(int i = 0; i < root.getChildren().size(); ++i){
			actionsString += root.getChildren().get(i).getActionFromParent();
			if(i < root.getChildren().size() - 1){
				actionsString += ", ";
			}
		}
		actionsString += "]";
		System.out.println("Playable actions in root: " + actionsString);*/

		int mctsIterations = 0;
		double iterations = 0.0;
		double timeTaken = 0.0;
		
		// because hasTimeLeft() checks for double the average simulation time, initializing this to 6.0 ensures 
		// we'll take a safety margin of 12 milliseconds for the first iteration, useful in cases where generateBreadthFirstTree
		// took a lot of time
		avgSimulationTime = 6.0;
		long elapsedMillisStart = elapsedForSimulationTimer.elapsedMillis();
		long lastElapsedMillis = elapsedMillisStart;
		long maxTimeMillis = elapsedForSimulationTimer.remainingTimeMillis() + lastElapsedMillis;

		//System.out.println("");
		//System.out.println("Time left before MCTS loop = " + elapsedForSimulationTimer.remainingTimeMillis());
		while (hasTimeLeft(elapsedForSimulationTimer, maxTimeMillis, lastElapsedMillis)){
			ONE_STEP_EVAL = 0.0;
			NUM_ADVANCE_OPS = 0;
			
			root.setStateObs(rootStateObs);

			// selection
			MctNode selectedNode = selectionStrategy.select(root, elapsedForSimulationTimer);
			
			// play-out
			MctNode playOutEnd = playoutStrategy.runPlayout(selectedNode, elapsedForSimulationTimer);
			
			//if(playOutEnd.getStateObs().isGameOver() && playOutEnd.getStateObs().getGameWinner() == Types.WINNER.PLAYER_LOSES){
			//	++TOTAL_LOSS_ITERATIONS;
			//}
			
			// backpropagation
			backup(playOutEnd, playoutEval.scorePlayout(playOutEnd.getStateObs()), elapsedForSimulationTimer, false);

			// set values for time management
			iterations += (NUM_ADVANCE_OPS / Math.max(10.0, playOutEnd.getDepth()));	// TODO use max playout depth instead of hardcoded 10.0
			++mctsIterations;
			++TOTAL_ITERATIONS;
			lastElapsedMillis = elapsedForSimulationTimer.elapsedMillis();
			timeTaken = lastElapsedMillis - elapsedMillisStart;
			
			if(iterations > 0.0){
				avgSimulationTime = timeTaken / iterations;
			}
			
			//System.out.println("Finished " + iterations + " iterations with avg time of " + avgSimulationTime);
			
			/*String avgScoresString = "[";
			for(int i = 0; i < root.getChildren().size(); ++i){
				avgScoresString += (root.getChildren().get(i).getTotalScore() / root.getChildren().get(i).getNumVisits());
				if(i < root.getChildren().size() - 1){
					avgScoresString += ", ";
				}
			}
			avgScoresString += "]";
			System.out.println("Avg. Scores after iteration " + iterations + ": " + avgScoresString);*/
		}
		
		/*System.out.println("Stopping MCTS with " + elapsedForSimulationTimer.remainingTimeMillis() + "ms left");
		System.out.println("num sims = " + iterations);
		System.out.println("time taken = " + timeTaken);
		System.out.println("avg. sim time = " + avgSimulationTime);
		System.out.println("maxTimeMillis = " + maxTimeMillis);
		System.out.println("lastElapsedMillis = " + lastElapsedMillis);
		System.out.println();*/

		MIN_ITERATIONS_PER_GAME = Math.min(mctsIterations, MIN_ITERATIONS_PER_GAME);
		MAX_ITERATIONS_PER_GAME = Math.max(mctsIterations, MAX_ITERATIONS_PER_GAME);
		
		//System.out.println("Ran " + iterations + " iterations");
		
		return moveSelectionStrategy.selectMove(root);
	}
	
	public void backup(MctNode playOutEnd, double[] scores, ElapsedCpuTimer elapsedForSimulationTimer, boolean inescapableLossFound){		
		if((scores[Agent.myID] == rootScore || alwaysKB) && !playOutEnd.getStateObs().isGameOver() && allowsKnowledgeBasedEvaluation()){
			scores[Agent.myID] += Globals.knowledgeBase.knowledgeBasedEval(playOutEnd.getStateObs());
		}
		
		scores[Agent.myID] += ONE_STEP_EVAL;
		
		for(int i = 0; i < scores.length; ++i){
			MAX_SCORE = Math.max(MAX_SCORE, scores[i]);
			MIN_SCORE = Math.min(MIN_SCORE, scores[i]);
		}
		
		MctNode updateNode = playOutEnd;
		while (updateNode != null) {			
			updateNode.backpropagate(scores[Agent.myID], scores[Agent.otherID]);
			
			if(updateNode.getParent() != null && collectActionStatistics){
				// we want to collect statistics for the played action
				ActionLocation action = updateNode.getActionLocationFromParent();
				
				Score actionScore = actionStatistics.get(action);
				if(actionScore == null){
					actionScore = new Score();
					actionStatistics.put(action, actionScore);
				}
				
				actionScore.score += scores[Agent.myID];
				actionScore.timesVisited += 1.0;
				
				MIN_ACTION_SCORE = Math.min(MIN_ACTION_SCORE, scores[Agent.myID]);
				MAX_ACTION_SCORE = Math.max(MAX_ACTION_SCORE, scores[Agent.myID]);
				
				if(maxActionNGramSize > 1){
					// we also want to collect statistics for n-grams
					for(int n = 2; n <= maxActionNGramSize; ++n){
						// create n-gram of size n
						ActionLocation[] nGram = new ActionLocation[n];
						MctNode currentActionNode = updateNode;
						boolean finishedNGram = true;
						
						for(int actionIdx = n - 1; actionIdx >= 0; --actionIdx){
							if(currentActionNode.getParent() == null){
								// tree is not deep enough for an n-gram of this size
								finishedNGram = false;
								break;
							}
							
							nGram[actionIdx] = currentActionNode.getActionLocationFromParent();
							currentActionNode = currentActionNode.getParent();
						}
						
						if(!finishedNGram){
							break;
						}
						
						ActionNGram actionNGram = new ActionNGram(nGram);
						Score actionNGramScore = actionNGramStatistics.get(actionNGram);
						if(actionNGramScore == null){
							actionNGramScore = new Score();
							actionNGramStatistics.put(actionNGram, actionNGramScore);
						}
						
						actionNGramScore.score += scores[Agent.myID];
						actionNGramScore.timesVisited += 1.0;
					}
				}
			}
			
			updateNode.postBackup();
			updateNode = updateNode.getParent();
		}
	}
	
	// TODO don't re-use tree if the avatar type changed? or maybe only if the set of legal actions changed?
	// we only generate child nodes the first time currently when we need children of a node, so we don't correctly
	// handle changed action sets
	public void decayOldTree(MctNode root){
		// decay the old results in all nodes of the tree
		ArrayList<MctNode> nodes = new ArrayList<MctNode>();
		nodes.add(root);
		
		//int nodesDecayed = 0;
		
		while(!nodes.isEmpty()){
			//++nodesDecayed;
			
			// pop a node from the list of nodes to process
			MctNode node = nodes.remove(nodes.size() - 1);
			
			// we'll also have to process all children of the current node
			nodes.addAll(node.getChildren());
			
			// decay this node
			node.decay(treeDecayFactor);
		}
		
		//System.out.println("Decayed " + nodesDecayed + " nodes!");
	}
	
	public void decrementTreeDepth(MctNode root){
		ArrayList<MctNode> nodes = new ArrayList<MctNode>();
		nodes.add(root);
				
		while(!nodes.isEmpty()){
			// pop a node from the list of nodes to process
			MctNode node = nodes.remove(nodes.size() - 1);
			
			// we'll also have to process all children of the current node
			nodes.addAll(node.getChildren());
			
			// decrement depth of the node
			node.decrementDepth();
		}
	}
	
	public boolean allowsKnowledgeBasedEvaluation(){
		return knowledgeBasedEval;
	}
	
	public boolean collectsActionStatistics(){
		return collectActionStatistics;
	}
	
	public void decayActionStatistics(double decayFactor){
		if(collectActionStatistics){
			for(Score actionScore : actionStatistics.values()){
				actionScore.decay(decayFactor);
			}
			
			if(maxActionNGramSize > 1){
				for(Score actionNGramScore : actionNGramStatistics.values()){
					actionNGramScore.decay(decayFactor);
				}
			}
		}
	}
	
	public MctNode getRoot(){
		return root;
	}
	
	/**
	 * Returns the score for playing the given action
	 * 
	 * @param action
	 * @return
	 */
	public Score getActionScore(ActionLocation action){
		Score score = actionStatistics.get(action);
		
		if(score == null){
			score = new Score();
			actionStatistics.put(action, score);
		}
		
		return score;
	}
	
	/**
	 * Returns the score for playing the given n-gram of actions
	 * 
	 * @param nGram
	 * @return
	 */
	public Score getActionNGramScore(ActionNGram nGram){
		Score score = actionNGramStatistics.get(nGram);
		
		if(score == null){
			score = new Score();
			actionNGramStatistics.put(nGram, score);
		}
		
		return score;
	}
	
	public double getRootEvaluation(){
		return rootScore;
	}
	
	private void generateBreadthFirstTree(StateObservationMulti rootStateObs, ElapsedCpuTimer elapsedTimer){
		root.setStateObs(rootStateObs);
		
		// create list of children of the root node with safety prepruning
		ArrayList<ACTIONS> availableActions = rootStateObs.getAvailableActions(Agent.myID);
		if(availableActions.size() > 1){
			availableActions.remove(ACTIONS.ACTION_NIL);
		}
		int numRootActions = availableActions.size();
		int[] numGameLosses = new int[numRootActions];
		
		// will keep sum of scores collected in this array so we can initialize MCTS node with an average score
		double[] scoreSums = new double[numRootActions];
		int numSafetyChecksCompleted = 0;
		
		// will keep generated state observations here so we can cache them in the nodes and re-use them later in MCTS
		@SuppressWarnings("unchecked")
		ArrayList<StateObservationMulti>[] stateObservations = new ArrayList[numRootActions];
		for(int i = 0; i < numRootActions; ++i){
			stateObservations[i] = new ArrayList<StateObservationMulti>(MAX_NUM_SAFETY_CHECKS);
		}
		
		long maxDurationSafetyCheckRound = 0L;
		for(int safetyCheckRound = 0; safetyCheckRound < MAX_NUM_SAFETY_CHECKS; ++safetyCheckRound){
			ElapsedCpuTimer roundTimer = new ElapsedCpuTimer(CompetitionParameters.TIMER_TYPE);
			for(int actionIdx = 0; actionIdx < numRootActions; ++actionIdx){				
				StateObservationMulti successor = rootStateObs.copy();
				successor.advance(Globals.generateActionArray(availableActions.get(actionIdx), successor, null, null, 0.0, root));
				
				if(successor.isGameOver() && successor.getMultiGameWinner()[Agent.myID] == WINNER.PLAYER_LOSES
						&& successor.getMultiGameWinner()[Agent.otherID] != WINNER.PLAYER_LOSES){
					numGameLosses[actionIdx] += 1;
				}
				
				scoreSums[actionIdx] += playoutEval.scorePlayout(successor)[Agent.myID];
				stateObservations[actionIdx].add(successor);
				
				if(safetyCheckRound == 0 && knowledgeBasedEval){
					// in the first round, also add KB eval
					scoreSums[actionIdx] += Globals.knowledgeBase.knowledgeBasedEval(successor);
				}
			}
			
			++numSafetyChecksCompleted;
			maxDurationSafetyCheckRound = Math.max(maxDurationSafetyCheckRound, roundTimer.elapsedMillis());
			long remainingMillis = elapsedTimer.remainingTimeMillis();
			
			if(remainingMillis < TIME_BUFFER_MILLISEC || remainingMillis < maxDurationSafetyCheckRound){
				break;
			}
		}
		
		int lowestNumGameLosses = MAX_NUM_SAFETY_CHECKS;
		TIntArrayList safeActions = new TIntArrayList(numRootActions, -1);
		for(int actionIdx = 0; actionIdx < numRootActions; ++actionIdx){
			if(numGameLosses[actionIdx] < lowestNumGameLosses){
				lowestNumGameLosses = numGameLosses[actionIdx];
				safeActions.resetQuick();
				safeActions.add(actionIdx);
			}
			else if(numGameLosses[actionIdx] == lowestNumGameLosses){
				safeActions.add(actionIdx);
			}
		}
		
		ArrayList<MctNode> newRootChildren = new ArrayList<MctNode>(safeActions.size());
		for(int i = 0; i < safeActions.size(); ++i){
			int safeActionIdx = safeActions.getQuick(i);
			ACTIONS action = availableActions.get(safeActionIdx);
			
			MctNode childNode = root.getExpandedChildForAction(action);
			if(childNode == null){
				childNode = new MctNode(root, action);
			}
			
			// we'll initialize the node with a single visit and the average score obtained among all safety checks
			double avgScore = scoreSums[safeActionIdx] / numSafetyChecksCompleted;
			childNode.backpropagate(avgScore, 0.0);
			
			// for every time that we pretend a child of the root node was visited, we also pretend that the root was visited
			// (this keeps the counters consistent)
			root.backpropagate(avgScore, 0.0);
			
			MAX_SCORE = Math.max(MAX_SCORE, avgScore);
			MIN_SCORE = Math.min(MIN_SCORE, avgScore);
			
			childNode.cacheStateObservations(stateObservations[safeActionIdx]);
			
			newRootChildren.add(childNode);
		}
		
		root.getUnexpandedActions().clear();
		root.getChildren().clear();
		root.getChildren().addAll(newRootChildren);
	} 
	
	/**
	 * Performs safety prepruning on the children of the given node. The node is expected to
	 * already be fully expanded
	 * 
	 * @param node
	 * @param state
	 */
	public void safetyPreprune(MctNode node, StateObservationMulti state, ElapsedCpuTimer elapsedTimer){
		node.setStateObs(state);
		
		ArrayList<MctNode> children = node.getChildren();
		int numActions = children.size();
		int[] numGameLosses = new int[numActions];
		
		// will keep generated state observations here so we can cache them in the nodes and re-use them later in MCTS
		@SuppressWarnings("unchecked")
		ArrayList<StateObservationMulti>[] stateObservations = new ArrayList[numActions];
		for(int i = 0; i < numActions; ++i){
			stateObservations[i] = new ArrayList<StateObservationMulti>(MAX_NUM_SAFETY_CHECKS);
		}

		long maxDurationSafetyCheckRound = 0L;
		for(int safetyCheckRound = 0; safetyCheckRound < MAX_NUM_SAFETY_CHECKS; ++safetyCheckRound){
			ElapsedCpuTimer roundTimer = new ElapsedCpuTimer(CompetitionParameters.TIMER_TYPE);
			for(int actionIdx = 0; actionIdx < numActions; ++actionIdx){				
				StateObservationMulti successor = state.copy();
				successor.advance(Globals.generateActionArray(children.get(actionIdx).getActionFromParent(), 
																successor, null, null, 0.0, root));	// TODO also use these states to update knowledge base?
				
				if(successor.isGameOver() && successor.getMultiGameWinner()[Agent.myID] == WINNER.PLAYER_LOSES
						&& successor.getMultiGameWinner()[Agent.otherID] != WINNER.PLAYER_LOSES){
					// in 2-player games, we'll only safety preprune if the opponent does not lose
					// we may sometimes want to force a loss for both players
					numGameLosses[actionIdx] += 1;
				}
				
				stateObservations[actionIdx].add(successor);
			}
			
			maxDurationSafetyCheckRound = Math.max(maxDurationSafetyCheckRound, roundTimer.elapsedMillis());
			long remainingMillis = elapsedTimer.remainingTimeMillis();
			
			if(remainingMillis < TIME_BUFFER_MILLISEC || remainingMillis < maxDurationSafetyCheckRound){
				break;
			}
		}
		
		int lowestNumGameLosses = MAX_NUM_SAFETY_CHECKS;
		for(int actionIdx = 0; actionIdx < numActions; ++actionIdx){
			if(numGameLosses[actionIdx] < lowestNumGameLosses){
				lowestNumGameLosses = numGameLosses[actionIdx];
			}
		}
		
		// collect indices of children that should be removed
		TIntArrayList toRemove = new TIntArrayList(numActions, -1);
		for(int i = 0; i < numActions; ++i){
			if(numGameLosses[i] > lowestNumGameLosses){
				toRemove.add(i);
			}
			else{
				// the child at index i is not gonna be removed, so we can cache the generated states for that node
				children.get(i).cacheStateObservations(stateObservations[i]);
			}
		}
		
		// remove the children at the indices that we just collected, in reverse order because that's most efficient
		for(int i = toRemove.size() - 1; i >= 0; --i){
			children.remove(toRemove.getQuick(i));
		}
	}
	
	public boolean hasTimeLeft(ElapsedCpuTimer timer, long maxTimeMillis, long elapsedMillis){
		long timeRemaining = maxTimeMillis - elapsedMillis;
		return (timeRemaining > 2.0 * avgSimulationTime 	&&
				timeRemaining > TIME_BUFFER_MILLISEC			);
	}
	
	public boolean hasTimeLeft(ElapsedCpuTimer timer){
		long timeRemaining = timer.remainingTimeMillis();
		return (timeRemaining > 2.0 * avgSimulationTime 	&&
				timeRemaining > TIME_BUFFER_MILLISEC			);
	}
}
